# -*- coding: utf-8 -*-
"""pretrained_models_benchmarking (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XuRTzJzJnePwtfDHIQ0eRZg6BN8r0k3y
"""

!pip install tensorflow==2.15.0
!pip install tf-keras==2.15.0
!pip install transformers==4.37.0
!pip install pandas

from google.colab import files
uploaded = files.upload()

import random
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelBinarizer
import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler

# Make reproducible
np.random.seed(1234)
tf.random.set_seed(1234)
random.seed(1234)

# Define a function to read the corpus
def read_corpus(corpus_file):
    documents = []
    labels = []
    with open(corpus_file, encoding='utf-8') as f:
        for line in f:
            tokens = line.strip()
            documents.append(" ".join(tokens.split()[3:]).strip())
            labels.append(tokens.split()[0])
    return documents, labels

# Function to tokenize data
def tokenize_data(tokenizer, data, max_length):
    return tokenizer(data, padding=True, max_length=max_length, truncation=True, return_tensors="np").data

# Define a learning rate schedule function
def lr_schedule(epoch, initial_lr=5e-5, drop=0.5, epochs_drop=5):
    return initial_lr * (drop ** (epoch // epochs_drop))

def experiment_with_model(model_name, max_length, batch_size, epochs):
    # Load the model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)

    # Read the data
    X_train, Y_train = read_corpus('train.txt')
    X_dev, Y_dev = read_corpus('dev.txt')

    # Transform string labels to one-hot encodings
    encoder = LabelBinarizer()
    Y_train_bin = encoder.fit_transform(Y_train)
    Y_dev_bin = encoder.fit_transform(Y_dev)

    # Tokenize the data
    tokens_train = tokenize_data(tokenizer, X_train, max_length)
    tokens_dev = tokenize_data(tokenizer, X_dev, max_length)

    # Compile the model
    loss_function = CategoricalCrossentropy(from_logits=True)
    optim = Adam(learning_rate=0.000002)
    model.compile(loss=loss_function, optimizer=optim, metrics=['accuracy'])

    # Learning rate scheduler
    lr_scheduler = LearningRateScheduler(lambda epoch: lr_schedule(epoch))

    # Train the model
    history = model.fit(tokens_train, Y_train_bin, validation_data=(tokens_dev, Y_dev_bin),
                        epochs=epochs, batch_size=batch_size, callbacks=[EarlyStopping(patience=5), lr_scheduler])

    # Evaluate the model on the dev set
    Y_pred = model.predict(tokens_dev)["logits"]
    
    Y_pred = np.argmax(Y_pred, axis=1)
    
    # Convert binarized Y_dev back to original labels
    Y_test = encoder.inverse_transform(Y_dev_bin)

    # Define the labels
    labels = ['books', 'camera', 'dvd', 'health', 'music', 'software']

    # Calculate the confusion matrix
    print('\n*** CONFUSION MATRIX ***')
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
    cm = confusion_matrix(Y_test, Y_pred, labels=labels)
    print(' '.join(labels))
    print(cm)

    # If --plot_show argument is given, display a plot of the confusion matrix
    if args.plot_show:
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
        disp.plot()
        import matplotlib.pyplot as plt
        plt.show()
    
    Y_dev_bin = np.argmax(Y_dev_bin, axis=1)
    accuracy = accuracy_score(Y_dev_bin, Y_pred)

    print(f'Accuracy on validation set with model {model_name}: {accuracy:.3f}')
    return history

# Example experiment
model_names = ["distilbert-base-uncased", "microsoft/deberta-v3-base", "bert-base-uncased", "roberta-base"]
results = {}

for model_name in model_names:
    print(f"Experimenting with model: {model_name}")
    history = experiment_with_model(model_name=model_name, max_length=128, batch_size=32, epochs=10)
    results[model_name] = history

import matplotlib.pyplot as plt
import pandas as pd

# Initialize a figure
plt.figure(figsize=(12, 6))

# Prepare data for plotting and table
results_data = {}
final_accuracies = {}

# Loop through each model and plot its validation accuracy
for model_name, history in results.items():
    # Plot with markers for better visibility
    plt.plot(history.history['val_accuracy'],
             label=model_name,
             marker='o',
             markersize=5,
             linewidth=2)

    # Store the final validation accuracy for the table
    final_accuracy = history.history['val_accuracy'][-1]
    final_accuracies[model_name] = final_accuracy

# Add title and labels
plt.title('Validation Accuracy for Different Models', fontsize=16)
plt.title('max sequence = 128, batch size = 32', fontsize=10)
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Validation Accuracy', fontsize=14)

# Add grid for better readability
plt.grid(True)

# Add legend with improved layout
plt.legend(title='Models', title_fontsize='13', fontsize='12')

# Add x and y limits for better visualization
plt.xlim(0, max([len(history.history['val_accuracy']) for history in results.values()]) - 1)
plt.ylim(0.9, 1)

# Save the figure as an image file (optional)
plt.savefig('validation_accuracy_plot.png', dpi=300)

# Create a DataFrame for final results
results_df = pd.DataFrame(final_accuracies.items(), columns=['Model Name', 'Final Validation Accuracy'])

# Sort the DataFrame by Final Validation Accuracy in descending order
results_df = results_df.sort_values(by='Final Validation Accuracy', ascending=False).reset_index(drop=True)

# Display the ranked table
print("\nFinal Validation Accuracy Results (Ranked):")
print(results_df)